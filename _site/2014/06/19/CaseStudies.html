<!doctype html>
<head>
	<meta charset="utf-8"></meta>
	<meta content="width=device-width, initial-scale=1.0" name="viewport"></meta>
	<meta content="A set of small, responsive CSS modules that you can use in every web project." name="description"></meta>
	
	<title>Case Studies</title>
	<link rel="stylesheet" href="http://yui.yahooapis.com/pure/0.5.0/pure-min.css">
	<!--[if lte IE 8]>
    <link rel="stylesheet" href="http://yui.yahooapis.com/pure/0.5.0/grids-responsive-old-ie-min.css">
<![endif]-->
<!--[if gt IE 8]><!-->
    <link rel="stylesheet" href="http://yui.yahooapis.com/pure/0.5.0/grids-responsive-min.css">
<!--<![endif]-->
	<link href='http://fonts.googleapis.com/css?family=Raleway:400,100,200' rel='stylesheet' type='text/css'>
	<link href="/css/style.css" rel="stylesheet"></link>
	<link href="/css/type.css" rel="stylesheet"></link>
	
	<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

	<script type="text/javascript"
	  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>

</head>
<body>
<div class="main">
<div id="bayesdb-logo"><a href=""><span class="t-bayes">Bayes</span><span class="t-db">DB</span></a></div>
	<div class="header">
		<div class="default-header-titles">
			<h1>Case Studies</h1>
			<h2></h2>
		</div>
	</div>

<div id="nav-bow">
	<div class="pure-g">
		<div class="pure-u-1 pure-u-md-1-5 pure-u-lg-1-5 nb-1"><div class="l-box-nb"><a href="/2014/06/18/GetStarted.html"><p>Get Started</p></a></div></div>
		<div class="pure-u-1 pure-u-md-1-5 pure-u-lg-1-5 nb-2"><div class="l-box-nb"><a href="/2014/06/19/Docs.html"><p>Documentation</p></a></div></div>
		<div class="pure-u-1 pure-u-md-1-5 pure-u-lg-1-5 nb-3"><div class="l-box-nb"><a href="/2014/06/19/CaseStudies.html"><p>Case Studies</p></a></div></div>
		<div class="pure-u-1 pure-u-md-1-5 pure-u-lg-1-5 nb-4"><div class="l-box-nb"><a href="/2014/06/19/GetInvolved.html"><p>Get Involved</p></a></div></div>
		<div class="pure-u-1 pure-u-md-1-5 pure-u-lg-1-5 nb-5"><div class="l-box-nb"><a href="/2014/06/18/FAQ.html"><p>FAQ / Help</p></a></div></div>
	</div>
</div>

<div class="layout">
	<h1 id="the_bayesian_approach_to_bob_ross">The Bayesian Approach to Bob Ross</h1>

<p>A few weeks ago, Walt Hickey of <a href="http://fivethirtyeight.com">fivethirtyeight.com</a> published a <a href="http://fivethirtyeight.com/features/a-statistical-analysis-of-the-work-of-bob-ross/">statistical analysis</a> of the career work of famed painter Bob Ross of the television show “The Joy of Painting.” His findings didn’t startle anyone who has seen at least a few episodes of the show, but were still interesting as a more systematic analysis of the full body of Ross’s output.</p>

<p>When we saw the findings, we couldn’t help but wonder whether BayesDB could tell us more, in particular:</p>

<ul>
<li>Would we see the same feature dependencies that Walt observed?</li>

<li>How would BayesDB’s row clustering compare to the k-means clusters?</li>

<li>Did the output from “The Joy of Painting” change over time?</li>

<li>What would a simulation of <em>new</em> Bob Ross paintings look like?</li>
</ul>

<p>Before going any further, we’d like to thank Walt Hickey and the folks at fivethirtyeight.com for making the data publicly available <a href="https://raw.githubusercontent.com/fivethirtyeight/data/master/bob-ross/elements-by-episode.csv">here</a>.</p>

<h2 id="replication">Replication</h2>

<p>Just to be sure we’re using exactly the same data, it’s worth replicating some empirical probabilities, like the fact that 85% of Bob Ross’s paintings included two or more trees, or that 60% of paintings with at least <em>one</em> mountain actually feature at least <em>two</em> mountains (a conditional probability, as Walt described). We used BQL’s <code>SUMMARIZE</code> statement, excluding episodes where a guest (such as Bob’s son Steve) actually painted:</p>

<pre><code>SUMMARIZE SELECT trees FROM bobross WHERE guest = 0</code></pre>
<table><thead><tr><th></th><th>trees</th></tr></thead><tbody><tr><td style="text-align: left;">type</td><td style="text-align: left;">multinomial</td></tr>
<tr><td style="text-align: left;">count</td><td style="text-align: left;">381</td></tr>
<tr><td style="text-align: left;">unique</td><td style="text-align: left;">2</td></tr>
<tr><td style="text-align: left;">mode1</td><td style="text-align: left;">1</td></tr>
<tr><td style="text-align: left;">mode2</td><td style="text-align: left;">0</td></tr>
<tr><td style="text-align: left;">prob_mode1</td><td style="text-align: left;">0.845144356955</td></tr>
<tr><td style="text-align: left;">prob_mode2</td><td style="text-align: left;">0.154855643045</td></tr>
</tbody></table>
<pre><code>SUMMARIZE SELECT mountains FROM bobross WHERE guest = 0 AND mountain = 1</code></pre>
<table><thead><tr><th></th><th>mountains</th></tr></thead><tbody><tr><td style="text-align: left;">type</td><td style="text-align: left;">multinomial</td></tr>
<tr><td style="text-align: left;">count</td><td style="text-align: left;">149</td></tr>
<tr><td style="text-align: left;">unique</td><td style="text-align: left;">2</td></tr>
<tr><td style="text-align: left;">mode1</td><td style="text-align: left;">1</td></tr>
<tr><td style="text-align: left;">mode2</td><td style="text-align: left;">0</td></tr>
<tr><td style="text-align: left;">prob_mode1</td><td style="text-align: left;">0.604026845638</td></tr>
<tr><td style="text-align: left;">prob_mode2</td><td style="text-align: left;">0.395973154362</td></tr>
</tbody></table>
<p>The results match Walt’s, so I’m convinced - let’s begin our Bayesian analysis of the data set.</p>

<h2 id="feature_dependence">Feature Dependence</h2>

<p>BayesDB has a statement to estimate the probability of a dependent relationship between two columns:</p>

<pre><code>ESTIMATE PAIRWISE DEPENDENCE PROBABILITY FOR bobross SAVE TO dep_prob_all.png</code></pre>

<p><img src="/assets/images/cs_bobross/dependence_probability_all.png" alt="dependence_probability_all" /></p>

<p>You can see that BayesDB picks up a high dependence probability for a lot of the relationships that Walt described, like tree/trees and mountain/mountains (emphasis on singular versus plural), snow/winter/cabin, clouds/cumulus, and beach/clouds/sun/waves/palm trees.</p>

<p>Most of the columns actually have very low probability of dependence, which is probably driven mainly by the fact that they occur so rarely. For example, just 1.5% of paintings feature any combination of a person, boat, mill, or lighthouse.</p>

<h2 id="clustering">Clustering</h2>

<p>Walt ran a k-means cluster analysis, a common technique to group items by their common characteristics. In BayesDB, we estimate <em>pairwise row similarity</em>, which is the similarity of every row with every other row based on back-end samples, and then save <em>connected components</em> as clusters.</p>

<pre><code>ESTIMATE PAIRWISE ROW SIMILARITY FROM bobross SAVE CONNECTED COMPONENTS WITH THRESHOLD 0.95 AS clusters</code></pre>

<p>We end up with 8 clusters. The image below shows the results of estimating pairwise row similarity:</p>

<p><img src="/assets/images/cs_bobross/pairwise_row_similarity_all.png" alt="pairwise_row_similarity_all" /></p>

<p>It’s not always clear exactly which features define clusters of rows, but one simple approach is to look at the columns with the highest probability within each cluster:</p>

<ol>
<li>deciduous, tree, trees</li>

<li>conifer, mountain, mountains, snow, snowy_mountain</li>

<li>clouds, conifer, lake, mountain, mountains, snowy_mountain, tree, trees</li>

<li>beach, clouds, ocean, rocks, waves</li>

<li>cabin, conifer, deciduous, mountain(s), structure, tree(s)</li>

<li>conifer, deciduous, snow, structure, tree(s), winter</li>

<li>barn, deciduous, grass, path, structure, tree(s)</li>

<li>cactus, clouds, guest, path, portrait</li>
</ol>

<p>Trees are the key feature of the first cluster, while snow and mountains make up the second. The third is a combination of trees, mountains, snow, and lakes, while the fourth seems to contain mainly beach/ocean scenes. The next three contain cabins, barns, or other structures in tree-filled settings, and the last cluster contains some rarities, like cacti (occuring 27.3% of the time in the cluster, versus 0.9% in the overall data) and guest painters (45.4% in the cluster, versus 5.5% in the overall data).</p>

<h2 id="nothing_stays_the_same">Nothing Stays the Same</h2>

<p>Did the characteristics of Bob Ross’s work change over time? We have data from 31 seasons - is a painting of a mountain more likely at the beginning of his career, the end, or does the probability remain relatively constant over the course of the show?</p>

<p>Let’s find the columns that are most probably dependent on which season of the show we observe:</p>

<pre><code>ESTIMATE COLUMNS OF bobross ORDER BY MUTUAL INFORMATION WITH season DESC LIMIT 6</code></pre>

<p>The resultings columns (excluding season) are clouds, cumulus, oval_frame, framed and cirrus.</p>

<p>Interesting, right? Did the likelihood of Bob Ross including clouds change over time, as well as the probability of framing a painting? This might be easier to confirm graphically. Here’s a plot with seasons 1-31 on the horizontal axis, and the distribution of paintings without clouds (0) and paintings with clouds (1) on the vertical axis:</p>

<pre><code>SELECT SCATTER clouds, season FROM bobross SAVE TO scatter_season_clouds.png</code></pre>

<p><img src="/assets/images/cs_bobross/scatter_season_clouds.png" alt="season versus clouds" /></p>

<p>You can see that the distribution of paintings with clouds is clearly pulled toward the earlier seasons of the show.</p>

<p>Let’s look at the same plot for season versus framed:</p>

<pre><code>SELECT SCATTER framed, season FROM bobross SAVE TO scatter_season_framed.png</code></pre>

<p><img src="/assets/images/cs_bobross/scatter_season_framed.png" alt="season versus framed" /></p>

<p>This time, you can clearly see that no paintings were framed in the first three seasons of the show, and also that the means aren’t aligned, with the mean season for framed paintings occurring several seasons after the mean for unframed paintings. Although Ross never framed more than 32% of his paintings in any given season, this is evidence that something changed about his preference (or his producer’s preference) for framed paintings.</p>

<h2 id="predictive_probability">Predictive Probability</h2>

<p>BayesDB is also equipped with tools to estimate the probability that each data cell takes its observed value - or any value, for that matter (not including values that are never observed in multinomial columns).</p>

<p>Let’s take a look at a summary of the predicted probabilities of snow being featured in a painting that features at least one mountain:</p>

<pre><code>SUMMARIZE SELECT PREDICTIVE PROBABILITY OF snow FROM bobross WHERE mountain = 1</code></pre>
<table><thead><tr><th></th><th>predictive probability of snow</th></tr></thead><tbody><tr><td style="text-align: left;">type</td><td style="text-align: left;">continuous</td></tr>
<tr><td style="text-align: left;">count</td><td style="text-align: left;">160.0</td></tr>
<tr><td style="text-align: left;">mean</td><td style="text-align: left;">0.864959932721</td></tr>
<tr><td style="text-align: left;">std</td><td style="text-align: left;">0.147145074073</td></tr>
<tr><td style="text-align: left;">min</td><td style="text-align: left;">0.126740093439</td></tr>
<tr><td style="text-align: left;">25%</td><td style="text-align: left;">0.907384611629</td></tr>
<tr><td style="text-align: left;">50%</td><td style="text-align: left;">0.933116488507</td></tr>
<tr><td style="text-align: left;">75%</td><td style="text-align: left;">0.93364806616</td></tr>
<tr><td style="text-align: left;">max</td><td style="text-align: left;">0.973474530055</td></tr>
</tbody></table>
<p>In a vast majority of paintings that feature one or more mountains, BayesDB would have been able to predict whether or not the painting would also feature snow. The probabilities are pretty strong in this case, averaging 86% with the median at 93%.</p>

<h2 id="the_unreleased_collection">The Unreleased Collection</h2>

<p>Using BayesDB’s SIMULATE statement, we can even simulate new observations (paintings) that have characteristics sampled from the estimated probability model.</p>

<p>We know that beach, rocks, and waves are probably dependent with ocean, so let’s try simulating 100 paintings and see how often those characteristics occur together when ocean = 1.</p>

<pre><code>SUMMARIZE SIMULATE beach, rocks, waves FROM bobross WHERE ocean = 1 TIMES 100</code></pre>
<table><thead><tr><th></th><th>beach</th><th>rocks</th><th>waves</th></tr></thead><tbody><tr><td style="text-align: left;">type</td><td style="text-align: left;">multinomial</td><td style="text-align: left;">multinomial</td><td style="text-align: left;">multinomial</td></tr>
<tr><td style="text-align: left;">count</td><td style="text-align: left;">100</td><td style="text-align: left;">100</td><td style="text-align: left;">100</td></tr>
<tr><td style="text-align: left;">unique</td><td style="text-align: left;">2</td><td style="text-align: left;">2</td><td style="text-align: left;">2</td></tr>
<tr><td style="text-align: left;">mode1</td><td style="text-align: left;">1</td><td style="text-align: left;">1</td><td style="text-align: left;">1</td></tr>
<tr><td style="text-align: left;">mode2</td><td style="text-align: left;">0</td><td style="text-align: left;">0</td><td style="text-align: left;">0</td></tr>
<tr><td style="text-align: left;">prob_mode1</td><td style="text-align: left;">0.68</td><td style="text-align: left;">0.52</td><td style="text-align: left;">0.75</td></tr>
<tr><td style="text-align: left;">prob_mode2</td><td style="text-align: left;">0.32</td><td style="text-align: left;">0.48</td><td style="text-align: left;">0.25</td></tr>
</tbody></table>
<p>So how did the simulation go? Well, of our 100 simulated ocean paintings, 68% featured a beach, 52% rocks, and 75% waves. Let’s compare that to the empirical values where ocean = 1, where 75%, 56%, and 94% of paintings have a beach, rocks, and waves, respectively. That’s reasonably close, especially when you consider that the empirical probabilites of those columns for the entire data set are 7%, 19%, and 8%, respectively.</p>

<p>It’s clear that BayesDB picked up on the relationship between those variables when simulating the new rows, so our 100 simulated Bob Ross paintings would look like, assuming they featured an ocean.</p>

<p>Now if only we had a statement that could generate images from these simulated rows, we could add <em>art forgery</em> to BayesDB’s list of useful applications. Maybe in the next release?</p>

<h2 id="notes_and_caveats">Notes and Caveats</h2>

<p>It sems there may be some incompleteness in the raw data, such as:</p>

<ol>
<li>There is a tag for ‘lakes’ (as opposed to the singular ‘lake’), that is always 0.</li>

<li>Sometimes paintings are coded 0 for a general feature, but 1 for a more specific feature that’s a subset of the more general feature. For example, 3 paintings are tagged ‘cumulus’ but not ‘clouds,’ and 1 painting is tagged ‘cirrus’ but not ‘clouds.’</li>
</ol>
<div class="layout">
		<div id="footer">
			<center><div class="footer-shadow"></div></center>
			
			<div class="footer-box"><div class="l-box">
			<p>BayesDB and its sister project, CrossCat, are being developed by <a href="http://web.mit.edu/jbaxter/www">Jay Baxter</a>, Dan Lovell, and <a href="http://web.mit.edu/vkm/www">Vikash Mansinghka</a> from the <a href="http://probcomp.csail.mit.edu/">MIT Probabilistic Computing Project</a> and by <a href="http://shaftolab.com/index.php/people/patrick-shafto/">Pat Shafto</a> and <a href="http://shaftolab.com/index.php/people/baxter-eaves/">Baxter Eaves</a> from the <a href="http://shaftolab.com/">CoCoSci Lab</a> at University of Louisville.</p>

		<p>Our work is generously supported by research a contract with DARPA (under the XDATA program) and the Google "Rethinking AI" project. The views expressed on this website and in our research are our own, and do not necessarily reflect the views of our government or corporate sponsors.</p>
			</div></div>
			<br>
			<p class="align-center">
				Copyright &copy; 2014 BayesDB
			</p>
			<p class="align-center">
				Follow us on <a href="#">Github</a>, <a href="#">Facbook</a>, and <a href="#">Twitter</a>.
			</p>
		</div>
	</div> <!-- end main -->
</body>